import { ChatGoogleGenerativeAI } from "@langchain/google-genai";
import { ChatOpenAI } from "@langchain/openai";
import { ChatGroq } from "@langchain/groq";
import { PromptTemplate } from "@langchain/core/prompts";
import { StringOutputParser } from "@langchain/core/output_parsers";
import { HumanMessage, SystemMessage } from "@langchain/core/messages";
import fs from 'fs';
import path from 'path';

// Sistema de m√∫ltiplas APIs com fallback autom√°tico via LangChain
class MultiAPIManager {
  constructor() {
    this.models = {};
    this.initializeModels();
  }

  initializeModels() {
    // Grok (Principal - temporariamente)
    if (process.env.GROK_API_KEY) {
      try {
        this.models.grok = new ChatGroq({
          apiKey: process.env.GROK_API_KEY,
          modelName: "mixtral-8x7b-32768", // Modelo mais est√°vel
          temperature: 0.7,
        });
        console.log("‚úÖ Grok API inicializada");
      } catch (error) {
        console.warn("‚ö†Ô∏è Erro ao inicializar Grok:", error.message);
      }
    }

    // Gemini (Desabilitado temporariamente devido a problemas de modelo)
    // if (process.env.GEMINI_API_KEY) {
    //   try {
    //     this.models.gemini = new ChatGoogleGenerativeAI({
    //       apiKey: process.env.GEMINI_API_KEY,
    //       modelName: "gemini-1.5-pro-latest",
    //       maxOutputTokens: 2048,
    //       temperature: 0.7,
    //     });
    //     console.log("‚úÖ Gemini API (LangChain) inicializada");
    //   } catch (error) {
    //     console.warn("‚ö†Ô∏è Erro ao inicializar Gemini:", error.message);
    //   }
    // }

    // OpenAI (Fallback)
    if (process.env.OPENAI_API_KEY) {
      try {
        this.models.openai = new ChatOpenAI({
          apiKey: process.env.OPENAI_API_KEY,
          modelName: "gpt-3.5-turbo",
          temperature: 0.7,
        });
        console.log("‚úÖ OpenAI API inicializada");
      } catch (error) {
        console.warn("‚ö†Ô∏è Erro ao inicializar OpenAI:", error.message);
      }
    }
  }

  getModel() {
    if (this.models.grok) return this.models.grok;
    if (this.models.gemini) return this.models.gemini;
    if (this.models.openai) return this.models.openai;
    // Mock para testes se n√£o houver chaves (evita crash imediato, mas falhar√° na chamada)
    console.warn("‚ö†Ô∏è Nenhuma API configurada. Usando mock para evitar crash.");
    return {
      invoke: async () => ({ content: "Erro: Nenhuma API de IA configurada." })
    };
  }
}

// Classe principal do Mega C√©rebro da Sara
export class SaraAI {
  constructor() {
    this.apiManager = new MultiAPIManager();
    this.loadPersonas();
  }

  loadPersonas() {
    try {
      const dataPath = path.join(process.cwd(), 'data');
      // Tenta carregar, se falhar usa defaults
      try {
        this.personas = {
          sara: JSON.parse(fs.readFileSync(path.join(dataPath, 'sara_personality.json'), 'utf8')),
          rackham: JSON.parse(fs.readFileSync(path.join(dataPath, 'persona_rackham.json'), 'utf8')),
          konrath: JSON.parse(fs.readFileSync(path.join(dataPath, 'persona_konrath.json'), 'utf8')),
          vaynerchuk: JSON.parse(fs.readFileSync(path.join(dataPath, 'persona_vaynerchuk.json'), 'utf8'))
        };
      } catch (e) {
        console.warn("‚ö†Ô∏è Arquivos de persona n√£o encontrados, usando defaults.");
        this.personas = {
          sara: { role: "Assistente de Marketing", methodology: "Inbound Marketing" },
          rackham: { role: "Especialista SPIN", methodology: "SPIN Selling" }
        };
      }
    } catch (error) {
      console.error("‚ùå Erro ao carregar personas:", error);
    }
  }

  // Processa mensagem com LangChain e Mem√≥ria
  async processMessage(userMessage, userInfo = {}, chatHistory = []) {
    try {
      const model = this.apiManager.getModel();
      
      // 1. Identificar o Especialista (Router)
      const expert = this.determineExpert(chatHistory, userMessage);
      
      // 2. Gerar Resposta T√©cnica do Especialista
      const expertResponse = await this.generateExpertResponse(model, expert, userMessage, userInfo, chatHistory);
      
      // 3. Humaniza√ß√£o da Sara (Tone Filter)
      const finalResponse = await this.applySaraFilter(model, expertResponse, expert, userInfo);

      return {
        success: true,
        response: finalResponse.response, // Texto para o usu√°rio
        conversationStage: expert,
        leadScore: this.calculateLeadScore(userInfo, chatHistory),
        nextAction: finalResponse.suggested_actions?.[0] || "continuar",
        activeAgent: expert,
        data: finalResponse // Objeto completo para o frontend
      };

    } catch (error) {
      console.error("Erro no processamento SaraAI:", error);
      return this.getFallbackResponse(userInfo);
    }
  }

  determineExpert(history, message) {
    // L√≥gica simples baseada no tamanho do hist√≥rico (pode ser melhorada com LLM)
    const msgCount = history.length;
    if (msgCount < 2) return "neil_rackham"; // In√≠cio: Investiga√ß√£o (SPIN)
    if (msgCount < 5) return "jill_konrath"; // Meio: Qualifica√ß√£o (BANT)
    return "gary_vaynerchuk"; // Fim: Fechamento (Value)
  }

  async generateExpertResponse(model, expertKey, message, userInfo, history) {
    const expertPersona = this.personas[expertKey.split('_')[1]] || this.personas.rackham;
    
    // Formata hist√≥rico para o prompt
    const historyText = history.map(h => `${h.role === 'user' ? 'Cliente' : 'Sara'}: ${h.content}`).join('\n');

    const prompt = PromptTemplate.fromTemplate(`
      Voc√™ √© {role}.
      Sua metodologia √©: {methodology}.
      
      CONTEXTO DO CLIENTE:
      Nome: {name}
      Neg√≥cio: {business}
      Tipo Projeto: {project}
      
      HIST√ìRICO DA CONVERSA:
      {history}
      
      CLIENTE DISSE: "{input}"
      
      Sua tarefa √© gerar uma resposta t√©cnica e estrat√©gica seguindo sua metodologia.
      N√£o se preocupe com o tom final, apenas com o conte√∫do estrat√©gico.
      
      RESPOSTA T√âCNICA:
    `);

    // Tenta com o modelo principal, se falhar usa fallback
    try {
      const systemPrompt = `Voc√™ √© ${expertPersona.role || "Especialista em Vendas"}.
Sua metodologia √©: ${expertPersona.methodology || "SPIN Selling"}.

CONTEXTO DO CLIENTE:
Nome: ${userInfo.nome || "Cliente"}
Neg√≥cio: ${userInfo.tipoNegocio || "N√£o informado"}
Tipo Projeto: ${userInfo.tipoServico || "Site"}

HIST√ìRICO DA CONVERSA:
${historyText}

Sua tarefa √© gerar uma resposta t√©cnica e estrat√©gica seguindo sua metodologia.
N√£o se preocupe com o tom final, apenas com o conte√∫do estrat√©gico.`;

      const messages = [
        new SystemMessage(systemPrompt),
        new HumanMessage(message)
      ];

      const response = await model.invoke(messages);
      return response.content;
    } catch (e) {
      console.error("Erro com modelo principal, tentando fallback:", e.message);
      
      // Tenta com Grok se dispon√≠vel
      if (this.apiManager.models.grok && model !== this.apiManager.models.grok) {
        try {
          const systemPrompt = `Voc√™ √© ${expertPersona.role || "Especialista em Vendas"}.
Sua metodologia √©: ${expertPersona.methodology || "SPIN Selling"}.

CONTEXTO DO CLIENTE:
Nome: ${userInfo.nome || "Cliente"}
Neg√≥cio: ${userInfo.tipoNegocio || "N√£o informado"}
Tipo Projeto: ${userInfo.tipoServico || "Site"}

HIST√ìRICO DA CONVERSA:
${historyText}

Sua tarefa √© gerar uma resposta t√©cnica e estrat√©gica seguindo sua metodologia.
N√£o se preocupe com o tom final, apenas com o conte√∫do estrat√©gico.`;

          const messages = [
            new SystemMessage(systemPrompt),
            new HumanMessage(message)
          ];

          const response = await this.apiManager.models.grok.invoke(messages);
          return response.content;
        } catch (fallbackError) {
          console.error("Erro tamb√©m no fallback:", fallbackError.message);
        }
      }
      
      return "Desculpe, estou analisando seu caso.";
    }
  }

  async applySaraFilter(model, draftResponse, expertUsed, userInfo) {
    const parser = new StringOutputParser();
    
    const systemPrompt = `
      Voc√™ √© a SARA, a IA especialista em marketing da Ronald Digital.
      Sua miss√£o √© traduzir a resposta t√©cnica de um especialista ({expert}) para uma linguagem humana, brasileira, carism√°tica e vendedora.
      
      DIRETRIZES DE TOM:
      - Use emojis pontuais (üòä, üöÄ, üí°).
      - Fale em primeira pessoa ("Eu acho...", "N√≥s podemos...").
      - Seja emp√°tica mas profissional.
      - NUNCA soe rob√≥tica ou corporativa demais.
      - Se o especialista sugeriu uma pergunta dura, suavize-a.
      
      DADOS DO CLIENTE:
      Nome: {name}
      
      RESPOSTA T√âCNICA ORIGINAL:
      "{draft}"
      
      SA√çDA ESPERADA (JSON):
      Retorne APENAS um JSON v√°lido com esta estrutura:
      {{
        "response": "Sua resposta reescrita aqui...",
        "sentiment": "positive/neutral/negative",
        "suggested_actions": ["A√ß√£o 1", "A√ß√£o 2"]
      }}
    `;

    const prompt = PromptTemplate.fromTemplate(systemPrompt);

    // Tenta com o modelo principal, se falhar usa fallback
    try {
      const systemPrompt = `Voc√™ √© a SARA, a IA especialista em marketing da Ronald Digital.
Sua miss√£o √© traduzir a resposta t√©cnica de um especialista (${expertUsed}) para uma linguagem humana, brasileira, carism√°tica e vendedora.

DIRETRIZES DE TOM:
- Use emojis pontuais (üòä, üöÄ, üí°).
- Fale em primeira pessoa ("Eu acho...", "N√≥s podemos...").
- Seja emp√°tica mas profissional.
- NUNCA soe rob√≥tica ou corporativa demais.
- Se o especialista sugeriu uma pergunta dura, suavize-a.

DADOS DO CLIENTE:
Nome: ${userInfo.nome || "Cliente"}

RESPOSTA T√âCNICA ORIGINAL:
"${draftResponse}"

SA√çDA ESPERADA (JSON):
Retorne APENAS um JSON v√°lido com esta estrutura:
{
  "response": "Sua resposta reescrita aqui...",
  "sentiment": "positive/neutral/negative",
  "suggested_actions": ["A√ß√£o 1", "A√ß√£o 2"]
}`;

      const messages = [
        new SystemMessage(systemPrompt),
        new HumanMessage("Reescreva a resposta t√©cnica seguindo as diretrizes.")
      ];

      const response = await model.invoke(messages);
      const resultStr = response.content;
      
      // Limpeza b√°sica para garantir JSON v√°lido (remove markdown ```json ... ```)
      const jsonStr = resultStr.replace(/```json/g, '').replace(/```/g, '').trim();
      return JSON.parse(jsonStr);
    } catch (e) {
      console.error("Erro com modelo principal no filtro Sara, tentando fallback:", e.message);
      
      // Tenta com Grok se dispon√≠vel
      if (this.apiManager.models.grok && model !== this.apiManager.models.grok) {
        try {
          const systemPrompt = `Voc√™ √© a SARA, a IA especialista em marketing da Ronald Digital.
Sua miss√£o √© traduzir a resposta t√©cnica de um especialista (${expertUsed}) para uma linguagem humana, brasileira, carism√°tica e vendedora.

DIRETRIZES DE TOM:
- Use emojis pontuais (üòä, üöÄ, üí°).
- Fale em primeira pessoa ("Eu acho...", "N√≥s podemos...").
- Seja emp√°tica mas profissional.
- NUNCA soe rob√≥tica ou corporativa demais.
- Se o especialista sugeriu uma pergunta dura, suavize-a.

DADOS DO CLIENTE:
Nome: ${userInfo.nome || "Cliente"}

RESPOSTA T√âCNICA ORIGINAL:
"${draftResponse}"

SA√çDA ESPERADA (JSON):
Retorne APENAS um JSON v√°lido com esta estrutura:
{
  "response": "Sua resposta reescrita aqui...",
  "sentiment": "positive/neutral/negative",
  "suggested_actions": ["A√ß√£o 1", "A√ß√£o 2"]
}`;

          const messages = [
            new SystemMessage(systemPrompt),
            new HumanMessage("Reescreva a resposta t√©cnica seguindo as diretrizes.")
          ];

          const response = await this.apiManager.models.grok.invoke(messages);
          const resultStr = response.content;
          
          const jsonStr = resultStr.replace(/```json/g, '').replace(/```/g, '').trim();
          return JSON.parse(jsonStr);
        } catch (fallbackError) {
          console.error("Erro tamb√©m no fallback do filtro Sara:", fallbackError.message);
        }
      }
      
      return {
        response: draftResponse, // Fallback para o texto original
        sentiment: "neutral",
        suggested_actions: []
      };
    }
  }

  calculateLeadScore(userInfo, history) {
    // Implementa√ß√£o simplificada de score
    let score = 0;
    if (userInfo.email) score += 1;
    if (userInfo.telefone) score += 1;
    if (history.length > 4) score += 1;
    if (userInfo.orcamento) score += 1;
    return Math.min(score, 4);
  }

  getFallbackResponse(userInfo) {
    return {
      success: true,
      response: `Oi ${userInfo.nome || ''}! Tive um pequeno lapso aqui, mas j√° voltei! üòÖ Como eu estava dizendo, podemos criar um projeto incr√≠vel para voc√™. Me conta mais sobre o que voc√™ precisa?`,
      conversationStage: "fallback",
      leadScore: 0,
      nextAction: "continuar",
      activeAgent: "sara_fallback",
      data: {
        response: `Oi ${userInfo.nome || ''}! Tive um pequeno lapso aqui, mas j√° voltei! üòÖ Como eu estava dizendo, podemos criar um projeto incr√≠vel para voc√™. Me conta mais sobre o que voc√™ precisa?`,
        sentiment: "neutral",
        suggested_actions: ["Falar sobre Projeto", "Ver Portf√≥lio"]
      }
    };
  }
}