// Sistema de Gerenciamento de MÃºltiplas APIs - VersÃ£o 2.0
// Suporte para Claude 3.5 Sonnet, GPT-4o, e Gemini 2.0 Flash

import { ChatAnthropic } from "@langchain/anthropic";
import { ChatOpenAI } from "@langchain/openai";
import { ChatGoogleGenerativeAI } from "@langchain/google-genai";
import { ChatGroq } from "@langchain/groq";

/**
 * Gerenciador moderno de APIs com modelos state-of-the-art
 *
 * Prioridade:
 * 1. Claude 3.5 Sonnet - Melhor em conversaÃ§Ã£o natural e humanizaÃ§Ã£o
 * 2. GPT-4o - Excelente balanceamento de qualidade e velocidade
 * 3. Gemini 2.0 Flash - RÃ¡pido e econÃ´mico
 * 4. Grok (Mixtral) - Fallback de Ãºltima instÃ¢ncia
 */
export class MultiAPIManagerV2 {
  constructor() {
    this.models = {};
    this.modelPriority = [];
    this.initializeModels();
  }

  /**
   * Inicializa todos os modelos disponÃ­veis baseado nas variÃ¡veis de ambiente
   */
  initializeModels() {
    // ðŸ¥‡ PRIORIDADE 1: Claude 3.5 Sonnet (Anthropic)
    // Melhor para: ConversaÃ§Ã£o natural, humanizaÃ§Ã£o, empatia
    if (process.env.ANTHROPIC_API_KEY) {
      try {
        this.models.claude = new ChatAnthropic({
          apiKey: process.env.ANTHROPIC_API_KEY,
          modelName: "claude-3-5-sonnet-20241022", // VersÃ£o mais recente
          temperature: 0.7,
          maxTokens: 1024,
        });
        this.modelPriority.push('claude');
        console.log("âœ… Claude 3.5 Sonnet inicializado (Prioridade 1)");
      } catch (error) {
        console.warn("âš ï¸ Erro ao inicializar Claude:", error.message);
      }
    }

    // ðŸ¥ˆ PRIORIDADE 2: GPT-4o (OpenAI)
    // Melhor para: AnÃ¡lise tÃ©cnica, raciocÃ­nio, velocidade
    if (process.env.OPENAI_API_KEY) {
      try {
        this.models.gpt4 = new ChatOpenAI({
          apiKey: process.env.OPENAI_API_KEY,
          modelName: "gpt-4o", // GPT-4 Optimized
          temperature: 0.7,
          maxTokens: 1024,
        });
        this.modelPriority.push('gpt4');
        console.log("âœ… GPT-4o inicializado (Prioridade 2)");
      } catch (error) {
        console.warn("âš ï¸ Erro ao inicializar GPT-4o:", error.message);
      }
    }

    // ðŸ¥‰ PRIORIDADE 3: Gemini 2.0 Flash (Google)
    // Melhor para: Respostas rÃ¡pidas, custo-benefÃ­cio
    if (process.env.GOOGLE_API_KEY) {
      try {
        this.models.gemini = new ChatGoogleGenerativeAI({
          apiKey: process.env.GOOGLE_API_KEY,
          modelName: "gemini-2.0-flash-exp", // VersÃ£o Flash experimental
          temperature: 0.7,
          maxOutputTokens: 1024,
        });
        this.modelPriority.push('gemini');
        console.log("âœ… Gemini 2.0 Flash inicializado (Prioridade 3)");
      } catch (error) {
        console.warn("âš ï¸ Erro ao inicializar Gemini:", error.message);
      }
    }

    // ðŸ”„ FALLBACK: Grok (Groq)
    // Mantido como Ãºltimo recurso para compatibilidade
    if (process.env.GROK_API_KEY) {
      try {
        this.models.grok = new ChatGroq({
          apiKey: process.env.GROK_API_KEY,
          modelName: "mixtral-8x7b-32768",
          temperature: 0.7,
        });
        this.modelPriority.push('grok');
        console.log("âœ… Grok (Mixtral) inicializado (Fallback)");
      } catch (error) {
        console.warn("âš ï¸ Erro ao inicializar Grok:", error.message);
      }
    }

    // ValidaÃ§Ã£o
    if (this.modelPriority.length === 0) {
      console.error("âŒ CRÃTICO: Nenhum modelo de IA foi inicializado!");
      console.error("Configure pelo menos uma das seguintes variÃ¡veis:");
      console.error("  - ANTHROPIC_API_KEY (Claude 3.5 Sonnet - Recomendado)");
      console.error("  - OPENAI_API_KEY (GPT-4o)");
      console.error("  - GOOGLE_API_KEY (Gemini 2.0 Flash)");
      console.error("  - GROK_API_KEY (Mixtral - Fallback)");
    } else {
      console.log(`\nðŸŽ¯ Modelos disponÃ­veis: ${this.modelPriority.join(', ')}`);
      console.log(`ðŸ† Modelo principal: ${this.modelPriority[0]}\n`);
    }
  }

  /**
   * Retorna o melhor modelo disponÃ­vel para conversaÃ§Ã£o geral
   * Prioriza Claude > GPT-4o > Gemini > Grok
   */
  getBestModel() {
    if (this.modelPriority.length === 0) {
      throw new Error("Nenhum modelo de IA disponÃ­vel. Configure as variÃ¡veis de ambiente.");
    }

    const modelKey = this.modelPriority[0];
    return this.models[modelKey];
  }

  /**
   * Retorna um modelo rÃ¡pido para tarefas simples (classificaÃ§Ã£o, anÃ¡lise de intenÃ§Ã£o)
   * Prioriza velocidade e custo
   */
  getFastModel() {
    // Prefere Gemini Flash para tarefas rÃ¡pidas
    if (this.models.gemini) return this.models.gemini;

    // Depois GPT-4o (tambÃ©m rÃ¡pido)
    if (this.models.gpt4) return this.models.gpt4;

    // Fallback para Grok
    if (this.models.grok) return this.models.grok;

    // Ãšltimo recurso: Claude (mais caro para tarefas simples)
    if (this.models.claude) return this.models.claude;

    throw new Error("Nenhum modelo disponÃ­vel");
  }

  /**
   * Retorna modelo especÃ­fico por nome
   * @param {string} modelName - 'claude' | 'gpt4' | 'gemini' | 'grok'
   */
  getSpecificModel(modelName) {
    if (!this.models[modelName]) {
      console.warn(`âš ï¸ Modelo ${modelName} nÃ£o disponÃ­vel. Usando fallback.`);
      return this.getBestModel();
    }
    return this.models[modelName];
  }

  /**
   * Tenta invocar o modelo com fallback automÃ¡tico em caso de erro
   * @param {Array} messages - Array de mensagens no formato LangChain
   * @param {Object} options - OpÃ§Ãµes adicionais
   */
  async invokeWithFallback(messages, options = {}) {
    const {
      preferredModel = null,
      maxRetries = 3,
      fastMode = false
    } = options;

    // Determina ordem de tentativa
    let modelsToTry;
    if (preferredModel && this.models[preferredModel]) {
      modelsToTry = [preferredModel, ...this.modelPriority.filter(m => m !== preferredModel)];
    } else if (fastMode) {
      modelsToTry = ['gemini', 'gpt4', 'grok', 'claude'].filter(m => this.models[m]);
    } else {
      modelsToTry = [...this.modelPriority];
    }

    let lastError = null;

    for (const modelKey of modelsToTry) {
      try {
        console.log(`ðŸ”„ Tentando com ${modelKey}...`);
        const model = this.models[modelKey];
        const response = await model.invoke(messages);
        console.log(`âœ… Resposta obtida com ${modelKey}`);

        return {
          success: true,
          content: response.content,
          modelUsed: modelKey,
          cost: this.estimateCost(modelKey, response.content)
        };
      } catch (error) {
        lastError = error;
        console.warn(`âš ï¸ Falha com ${modelKey}: ${error.message}`);

        // Se Ã© erro de rate limit, aguarda antes de tentar prÃ³ximo
        if (error.message.includes('rate') || error.message.includes('429')) {
          await this.sleep(2000);
        }
      }
    }

    // Se chegou aqui, todos os modelos falharam
    throw new Error(`Todos os modelos falharam. Ãšltimo erro: ${lastError?.message}`);
  }

  /**
   * Estima custo aproximado da chamada
   * @param {string} modelKey - Chave do modelo
   * @param {string} content - ConteÃºdo da resposta
   */
  estimateCost(modelKey, content) {
    const outputTokens = Math.ceil(content.length / 4); // Estimativa aproximada

    const costs = {
      claude: outputTokens * 0.000015,  // $15 / 1M tokens
      gpt4: outputTokens * 0.00001,     // $10 / 1M tokens
      gemini: outputTokens * 0.0000002, // $0.20 / 1M tokens
      grok: outputTokens * 0.000001     // $1 / 1M tokens (estimado)
    };

    return costs[modelKey] || 0;
  }

  /**
   * UtilitÃ¡rio para aguardar (rate limiting)
   */
  sleep(ms) {
    return new Promise(resolve => setTimeout(resolve, ms));
  }

  /**
   * Retorna estatÃ­sticas dos modelos disponÃ­veis
   */
  getStats() {
    return {
      totalModels: Object.keys(this.models).length,
      availableModels: this.modelPriority,
      primaryModel: this.modelPriority[0] || 'none',
      hasModernModel: !!(this.models.claude || this.models.gpt4 || this.models.gemini)
    };
  }

  /**
   * Valida se pelo menos um modelo moderno estÃ¡ configurado
   */
  hasModernModels() {
    return !!(this.models.claude || this.models.gpt4 || this.models.gemini);
  }
}

// Mock para testes sem API keys
export class MockAPIManager {
  getBestModel() {
    return {
      invoke: async () => ({
        content: JSON.stringify({
          resposta: "Sistema em modo de teste. Configure as APIs para funcionamento completo.",
          dados_extraidos: {},
          lead_score: 0,
          proxima_acao: "configurar_apis",
          metodologia_aplicada: "mock"
        })
      })
    };
  }

  getFastModel() {
    return this.getBestModel();
  }

  async invokeWithFallback() {
    return {
      success: true,
      content: "Modo mock ativo",
      modelUsed: "mock",
      cost: 0
    };
  }

  hasModernModels() {
    return false;
  }
}
